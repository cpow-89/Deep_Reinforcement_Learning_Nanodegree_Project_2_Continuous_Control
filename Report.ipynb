{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning Nanodegree: Project 2 - Continuous Control - Report\n",
    "\n",
    "\n",
    "### 1. General:\n",
    "\n",
    "The goal of this project was to train an agent, represented by a double-jointed arm, to maintain its position at the target location(great green sphere) for as many time steps as possible. \n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "<br>\n",
    "Random Agent:\n",
    "\n",
    "[image1]: https://raw.githubusercontent.com/cpow-89/Deep_Reinforcement_Learning_Nanodegree_Project_2_Continuous_Control/master/images/untrained_agent.gif?token=AmwnwlXyXniU-umlY4BNx8VSfAnYd57mks5bxNYIwA%3D%3D \"Random Agent\"\n",
    "\n",
    "![Random Agent][image1]\n",
    "\n",
    "### 2. Learning algorithm\n",
    "\n",
    "General Information:\n",
    "\n",
    "The used learning algorithm is called Deep Deterministic Policy Gradient(DDPPG) and was introduced in the \"Continuous control with deep reinforcement learning\" paper by Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra.\n",
    "They adopt the ideas underlying the success of Deep Q-Learning to the continuous action domain. \n",
    "The algorithm is often classified as an \"Actor-Critic\" method, but it can also be classified as a DQN method for continuous action spaces.\n",
    "The reason for this is that the critic network in DDPG is used to approximate the maximizer over the q values\n",
    "of the next state and not as a learned baseline as in other \"Actor-Critic\" methods.\n",
    "\n",
    "Intuition:\n",
    "\n",
    "- we use two deep neural networks(one the representing the actor and one serving the critic)\n",
    "    - we also use a copy of each network as a target network to get a more stable learning phase\n",
    "- the actor is used to approximate the optimal policy deterministically\n",
    "    - the actor always outputs the best-believed action for a given state\n",
    "    - the actor is basically learning $argmax_aQ(s,a)$\n",
    "- the critic learns to evaluate the optimal action-value function by using the actor's best-believed action\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "- initialize replay buffer $R$\n",
    "- initialize a random process $N$ for action exploration (Ornstein Uhlenbeck Noise)\n",
    "- set up agent\n",
    "    - register replay buffer $R$ and random process $N$\n",
    "    - randomly initialize actor network $Î¼(s|\\theta^\\mu)$  with weights $\\theta^\\mu$\n",
    "    - initialize actor target network $Q'$ with weights $\\theta'^\\mu \\leftarrow \\theta^\\mu$\n",
    "    - randomly initialize critic network $Q(s,a|\\theta^Q)$ with weights $\\theta^Q$\n",
    "    - initialize critic target network $Q'$ with weights $\\theta'^Q \\leftarrow \\theta^Q$\n",
    "\n",
    "- for episode = 1, max_number_of_episodes do:\n",
    "    - reset random process $N$\n",
    "    - receive initial observation state $s_1$\n",
    "    - for t = 1, T do:\n",
    "        - select action $a_t = \\mu(s_t|\\theta^\\mu ) + N_t$ according to the current policy and exploration noise\n",
    "        - execute action $a_t$ and observe reward $r_t$ and observe new state $s_{t+1}$\n",
    "        - store transition $(s_t, a_t, r_t, s_{t+1} )$ in $R$\n",
    "        - sample a random minibatch of $N$ transitions $(s_i, a_i, r_i, s_{i+1})$ from $R$\n",
    "        - set $y_i = r_i + \\gamma * Q'(s_{i+1}, \\mu'(s_{i+1}|\\theta^{\\mu'})|\\theta^{Q'})$\n",
    "        - update critic by minimizing the loss\n",
    "        - update the actor policy using the sampled policy gradient\n",
    "        - soft update the target networks\n",
    "    - end for\n",
    "- end for\n",
    "\n",
    "### 3. Hyperparameters\n",
    "- hyperparameters can be found in the config file\n",
    "\n",
    "buffer_size: 1000000 \n",
    "- the number of experience tuples we can save to our experience replay buffer\n",
    "- this value should be high to save as much experience as possible\n",
    "\n",
    "batch_size: 124\n",
    "- number of states, actions, rewards, next_states, dones tuples sampled from the experience buffer during training\n",
    "\n",
    "n_inputs: 33 \n",
    "- number of signals in the input vector\n",
    "\n",
    "n_actions: 4 \n",
    "- number of signals in the action vector\n",
    "\n",
    "gamma: 0.99\n",
    "- a decay factor for future rewards meaning received rewards currently should have more value than uncertain future rewards\n",
    "- the value should be close to 1 cause we only took one step into the future into account\n",
    "- hyperparameter was chosen according to Part 7: Experiment Details in the \"Continuous control with deep reinforcement learning\" paper \n",
    "\n",
    "tau: 0.001\n",
    "- value determines the step size of the soft network to target weight update\n",
    "- the value should be close to 0 to get a more stable learning process\n",
    "- hyperparameter was chosen according to Part 7: Experiment Details in the \"Continuous control with deep reinforcement learning\" paper \n",
    "\n",
    "learning_rate_actor: 0.0001\n",
    "- the rate at which the actor-network is updated(how big are the weight update steps)\n",
    "    - huge values lead to fast learning but will probably overshoot the optimum\n",
    "    - small values might lead to very slow learning\n",
    "    - hyperparameter was chosen according to Part 7: Experiment Details in the \"Continuous control with deep reinforcement learning\" paper \n",
    "    \n",
    "fc_units_actor: 400, 300\n",
    "- units for the fc layers in the actor-network\n",
    "- hyperparameter was chosen according to Part 7: Experiment Details in the \"Continuous control with deep reinforcement learning\" paper \n",
    "\n",
    "learning_rate_critic: 0.0003\n",
    "- the rate at which the critic network is updated(how big are the weight update steps)\n",
    "    - huge values lead to fast learning but will probably overshoot the optimum\n",
    "    - small values might lead to very slow learning\n",
    "    - hyperparameter was chosen according to Part 7: Experiment Details in the \"Continuous control with deep reinforcement learning\" paper \n",
    "    \n",
    "fc_units_critic: [400, 300]\n",
    "- units for the fc layers in the critic network\n",
    "- hyperparameter was chosen according to Part 7: Experiment Details in the \"Continuous control with deep reinforcement learning\" paper \n",
    "\n",
    "l2_weight_decay: 0.01\n",
    "- hyperparameter was chosen according to Part 7: Experiment Details in the \"Continuous control with deep reinforcement learning\" paper \n",
    "\n",
    "Ornstein Uhlenbeck Noise:\n",
    "\"mu\": 0\n",
    "- hyperparameter was chosen according to Part 7: Experiment Details in the \"Continuous control with deep reinforcement learning\" paper \n",
    "\n",
    "\"theta\": 0.15\n",
    "- hyperparameter was chosen according to Part 7: Experiment Details in the \"Continuous control with deep reinforcement learning\" paper \n",
    "\n",
    "\"sigma\": 0.2\n",
    "- hyperparameter was chosen according to Part 7: Experiment Details in the \"Continuous control with deep reinforcement learning\" paper \n",
    "\n",
    "\n",
    "### 4. Network architectures\n",
    "\n",
    "Critic + Critic_Target:\n",
    "\n",
    "DDPGCritic(<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(state_head): Sequential(<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0): Linear(in_features=33, out_features=400, bias=True)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1): ReLU()<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(state_action_body): Sequential(<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0): Linear(in_features=404, out_features=300, bias=True)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1): ReLU()<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2): Linear(in_features=300, out_features=1, bias=True)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;)<br>\n",
    ")<br>\n",
    "\n",
    "Actor + Actor_Target:\n",
    "\n",
    "DDPGActor(<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(network): Sequential(<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0): Linear(in_features=33, out_features=400, bias=True)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1): ReLU()<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2): Linear(in_features=400, out_features=300, bias=True)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(3): ReLU()<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(4): Linear(in_features=300, out_features=4, bias=True)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(5): Tanh()<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;)<br>\n",
    ")<br>\n",
    "\n",
    "### 5. Results\n",
    "\n",
    "Trained Agent:\n",
    "\n",
    "[image2]: https://raw.githubusercontent.com/cpow-89/Deep_Reinforcement_Learning_Nanodegree_Project_2_Continuous_Control/master/images/trained_agent.gif?token=Amwnwv58uwb_JY6Z0p0_vJrWmnnl-0Eeks5bxNVywA%3D%3D \"Trained Agent\"\n",
    "![Trained Agent][image2]\n",
    "\n",
    "\n",
    "### 6. Ideas for Future Work\n",
    "- add Prioritized Experience Replay and use the weight initialization suggested in the original DDPG paper\n",
    "    - should lead to faster and more stable learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
